{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2w3j3nP-g0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "55f23e61-aa7d-4d4b-a911-d8fa7295403c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import unicodedata\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "os.chdir('/gdrive/My Drive/nmt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgpdQXGCQ8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "cc4f3c93-108b-4d2a-c67d-2272a925656f"
      },
      "source": [
        "!pip install import_ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=a893ec0b747fe688d68e6c689ad143f88bba98d43a3b62ebc3cb7363873d4e3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4bz3DEPYF_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enc_dec import Encoder, BahdanauAttention , Decoder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84qNokFYN3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"spa-eng/spa-eng/spa.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWWFtuhAlBH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)# creating a space between a word and the punctuation following it\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)# replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "\n",
        "  w = w.strip()\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYdcT7iak794",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f0d24103-52ef-432e-f381-ffb41584da6b"
      },
      "source": [
        "en_sentence = u\"What would you like to drink?\"\n",
        "sp_sentence = \"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> What would you like to drink ? <end>\n",
            "b'<start> \\xc2\\xbf Puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYq6WRpWl79W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding = 'UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(x) for x in l.split('\\t')] for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CunEuRRBFPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksxV_dwJMrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples = None):\n",
        "  inp_lang, targ_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyei4W_TMlzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path, num_examples)\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wphpyxDqNvUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "994d8206-25ea-417f-f131-9606e31064cb"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N3bi5vDJs8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vKkRQsSUX9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aaad8eee-790a-4cf5-de68-0508fa5b914b"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "56 ----> please\n",
            "94 ----> take\n",
            "9 ----> a\n",
            "698 ----> bath\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "25 ----> por\n",
            "55 ----> favor\n",
            "426 ----> ba\n",
            "4314 ----> ate\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwsrqd5BUiCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_input_size = len(inp_lang.index_word) + 1\n",
        "vocab_target_size = len(targ_lang.index_word) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder =True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qVrgxZbfADk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b06fd79-13f0-47ba-85d3-1a07f6ad3ad5"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 11]), TensorShape([64, 18]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3nVhqMefYn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3caf306d-4c06-4faa-ed42-63a2579b243a"
      },
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden = tf.zeros((BATCH_SIZE, units))\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 11, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USiuJy9ptr9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d0f84d88-1b9d-40e8-c71d-1fb59a771062"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 11, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeT7TwBl0Jtf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2d7ee60-0eb7-478b-f504-dda1873ee443"
      },
      "source": [
        "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _ , _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 8895)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHLQ8ILnJ599",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype = loss.dtype)\n",
        "  loss*=mask\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVTZDb2TtvTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:,t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIYZ5f_FDcW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "587a86a5-90e3-4b85-a8a0-54f0a91fb85a"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = tf.zeros((BATCH_SIZE, units))\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if ((batch%100) == 0):\n",
        "      print(\"Epoch {}  Batch {}  Loss{:.4f}\".format(epoch + 1, batch, total_loss.numpy()))\n",
        "\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1  Batch 0  Loss3.0154\n",
            "Epoch 1  Batch 100  Loss179.2935\n",
            "Epoch 1  Batch 200  Loss330.5957\n",
            "Epoch 1  Batch 300  Loss470.1703\n",
            "Epoch 1 Loss 1.5074\n",
            "Time taken for 1 epoch 63.38821816444397 sec\n",
            "\n",
            "Epoch 2  Batch 0  Loss1.1754\n",
            "Epoch 2  Batch 100  Loss116.4235\n",
            "Epoch 2  Batch 200  Loss225.9061\n",
            "Epoch 2  Batch 300  Loss327.1769\n",
            "Epoch 2 Loss 1.0651\n",
            "Time taken for 1 epoch 46.74494242668152 sec\n",
            "\n",
            "Epoch 3  Batch 0  Loss0.8463\n",
            "Epoch 3  Batch 100  Loss85.1226\n",
            "Epoch 3  Batch 200  Loss166.9136\n",
            "Epoch 3  Batch 300  Loss245.8074\n",
            "Epoch 3 Loss 0.8031\n",
            "Time taken for 1 epoch 46.56194472312927 sec\n",
            "\n",
            "Epoch 4  Batch 0  Loss0.7033\n",
            "Epoch 4  Batch 100  Loss61.9215\n",
            "Epoch 4  Batch 200  Loss122.6227\n",
            "Epoch 4  Batch 300  Loss181.1514\n",
            "Epoch 4 Loss 0.5992\n",
            "Time taken for 1 epoch 46.607561349868774 sec\n",
            "\n",
            "Epoch 5  Batch 0  Loss0.4731\n",
            "Epoch 5  Batch 100  Loss44.0127\n",
            "Epoch 5  Batch 200  Loss88.3957\n",
            "Epoch 5  Batch 300  Loss132.4809\n",
            "Epoch 5 Loss 0.4397\n",
            "Time taken for 1 epoch 46.67987251281738 sec\n",
            "\n",
            "Epoch 6  Batch 0  Loss0.2967\n",
            "Epoch 6  Batch 100  Loss31.0055\n",
            "Epoch 6  Batch 200  Loss63.1994\n",
            "Epoch 6  Batch 300  Loss96.0462\n",
            "Epoch 6 Loss 0.3228\n",
            "Time taken for 1 epoch 46.86491131782532 sec\n",
            "\n",
            "Epoch 7  Batch 0  Loss0.2443\n",
            "Epoch 7  Batch 100  Loss22.3547\n",
            "Epoch 7  Batch 200  Loss46.2940\n",
            "Epoch 7  Batch 300  Loss71.1919\n",
            "Epoch 7 Loss 0.2388\n",
            "Time taken for 1 epoch 46.84209585189819 sec\n",
            "\n",
            "Epoch 8  Batch 0  Loss0.1739\n",
            "Epoch 8  Batch 100  Loss17.1461\n",
            "Epoch 8  Batch 200  Loss35.2079\n",
            "Epoch 8  Batch 300  Loss54.0416\n",
            "Epoch 8 Loss 0.1833\n",
            "Time taken for 1 epoch 46.71791481971741 sec\n",
            "\n",
            "Epoch 9  Batch 0  Loss0.1281\n",
            "Epoch 9  Batch 100  Loss13.3905\n",
            "Epoch 9  Batch 200  Loss27.7025\n",
            "Epoch 9  Batch 300  Loss42.9134\n",
            "Epoch 9 Loss 0.1460\n",
            "Time taken for 1 epoch 46.72296953201294 sec\n",
            "\n",
            "Epoch 10  Batch 0  Loss0.0701\n",
            "Epoch 10  Batch 100  Loss10.7463\n",
            "Epoch 10  Batch 200  Loss22.3925\n",
            "Epoch 10  Batch 300  Loss35.2215\n",
            "Epoch 10 Loss 0.1209\n",
            "Time taken for 1 epoch 46.90368342399597 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV36HFFTovoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.save_weights('encoder.h5')\n",
        "decoder.save_weights('decoder.h5')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI577UhiUj76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inplang_json = inp_lang.to_json()\n",
        "with open(\"inp_lang.json\", \"w\") as json_file:\n",
        "    json_file.write(inplang_json)\n",
        "\n",
        "targlang_json = targ_lang.to_json()\n",
        "with open(\"targ_lang.json\", \"w\") as json_file:\n",
        "    json_file.write(targlang_json)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGIYCwFg3Z3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = [preprocess_sentence(sentence)]\n",
        "  inputs = inp_lang.texts_to_sequences(sentence)\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen = max_length_inp, padding='post')\n",
        "\n",
        "  hidden = tf.zeros((1, units))\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "  \n",
        "  result = ''\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpAEl-pSDP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentences = evaluate(sentence)\n",
        "\n",
        "  print(\"Input : %s\" % (sentence))\n",
        "  print(\"Predicted Translate : {}\".format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Ft0nvFDx_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40509a24-d61e-4a33-e876-763eead26b32"
      },
      "source": [
        "translate('Do not Run!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : Do not Run!\n",
            "Predicted Translate : no corras ! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed6t7I1kD6q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}